# Infrastructure Specification for Integration tests

## Goal
To specify the interfaces between different components and the iteractions involved in executing an integration test. More concretly, proposing a spec for a Test Executive (pretty close to the one defined [here](https://github.com/CodaProtocol/coda/pull/4715). The role of a test executive is to bring together various components involved in executing an integration test and coordinating the interactions among them.
These components are 
    1. Testnet infrastructure: Given a network configuration, deploys a testnet for a specific infrastruture.
    2. Test: Defining the integration test itself
    3. Logging: Consuming logs from the deployed testnet for validation
    4. Automated validation: Defines and executes network-wide validations (Out of scope)

The goal is to have a system where each of these components can be independently implemented and the test executive will be responsible for intializing them and facilitating the interactions.

## Design

### Existing

Integrations tests are currently run on a simulated network using local processes. The test controls and inspects the nodes by communicating over RPC. The tests are tightly coupled to the simulated network and cannot be ported to run on cloud-based or container-based environments. More information on the motivation is described in this [rfc](https://github.com/CodaProtocol/coda/pull/4715) (TODO: Copy it to make it easier to read?)

### Improvements

The proposal for the test executive is part of the whole integration test framework revamp to address the drawbacks of the existing framework (#4735).

### Proposal

#### Infrastructure configuration

#### Test executive

The test executive requires the following inputs to succesfully execute an integration test

1. Genesis constants and ledger: 
`Runtime_config.t` currently has all the configurable constants and ledger 

A ledger can be specified in any of the following way: (already implemented)
```ocaml

module Accounts = struct
  type single =
    { pk: string option [@default None]
    ; sk: string option [@default None]
    ; balance: Currency.Balance.t
    ; delegate: string option [@default None] }
end

module Ledger = struct
  type base =
    | Named of string  (** One of the named ledgers in [Genesis_ledger] *)
    | Accounts of Accounts.t  (** A ledger generated from the given accounts *)
    | Hash of string  (** The ledger with the given root hash *)
end

```
Therefore, one could either use compiled ledgers or generate a new json list using the Testnet SDK from coda_automation.  [TODO: can be used to generate the json list with an option to include private keys. Since these are interation tests and transaction would be sent by different components]

2. Network config: the configuration for the network the integration test will run on

```ocaml

(*flags that we may want to specify for each daemon*)

type block_producer_flags = {
      coinbase_receiver: Public_key.Compressed.t
}

type snark_coordinator_flags = {
      snark_worker_fee: Fee.t
    ; work_reassignment_wait: int
    ; work_selection: `Seq | `Random
}
(* The block producer keys would be 1 to [count] in the ledger and any non-default flags*)
type block_producers = {count : int; flags: block_producer_flags list }

type archive_nodes = {count: int}

(*Snark coordinator keys will be from [block_producers.count]+1 to [block_producers.count]+[count]. Each snark coordinator is associated with a key and a fee. All the proofs generated by the workers connected to the coordinator is associated with that key and the fee. Each coordinator has a set of workers that would get work from the coordinator and submit proofs for the same.*)
type snark_coordinator = { count : int; with_unique_keys: int; workers_per_coordinator: int; flags: snark_coordinator_flags list }

type t = {
      block_producers: block_producers
    ; snark_coordinators: snark_coordinators
    ; archive_nodes: archive_nodes
    ; env: `Local | `Cloud }

```

3. Integration test: This is the program that defines the integration test written in a custom DSL (#4766)
The inputs to the test executive (network config and runtime config) can be passed to this program


A test executive will interact with two main components as shown in the image :

1. Network manager
2. Log engine

<img src="../res/test_exec.png" alt="drawing" width="500"/>

##### Network Manager

A network manager is basically a wrapper around the network infrastructure using which a testnet is deployed. The network manager implemented in ocaml would take as input network configuration and use it to generate required configuration for the deployment and deploy a testnet. The network manager is responsible for creating the seed nodes that other nodes can connect to. A separate network manager would be created for each environment where we want to run the testnet. The choice of the a network manager is made by the test executive. The interface between a network manager and the test executive is a module that the network manager would generate. This module `Testnet` exposes all the nodes and the commands that can be issued to the nodes.

```ocaml

module Node : sig
    type t

    val node_id_json : t -> json

    val start : t -> unit Deferred.t

    val stop : t -> unit Deferred.t

    ...
end


module Testnet = struct

    type t =
        { 
        testnet_id : json
        ; block_producers : Node list
        ; snark_coordinators: Node list
        ; archive_nodes: Node list}
end
```

A node will encapsulate all the information needed to connect to it and issue commands (if we want to use cli commands then we'll have to send the ip of the test executive to client-whitelist). In the current testnet infrastucture, a network manager would generate the terraform configuration files and deploy using terraform. There's no ocaml wrapper for terraform so we would have to write one.
To run it locally, using minikube perhaps, we would create another network manager that deploys a testnet locally.

The network manager itself can be specified as:

```ocaml
val deploy : Network_config.t -> Daemon_config.t -> Testnet.t Deferred.t

val destroy : Testnet.t -> unit Deferred.t
```

##### Log engine

Similar to the network manager, a log engine is also a wrapper around the logging infrastructure we'd use. The log engine will provide query functions to extract events from structured logs. Log engine queries can be used for node specific or network wide querying by choosing specific filters.
Again, for each logging infrastructure there will be a log engine that has custom implementation for the specific logging application used.

TODO: Example stack driver queries

The interface will basically need to have all the queries the test DSL would expose

Here's a rough structure of the test executive and how the it links different components

Assumption: (as a CI step?)
1. generate and upload the docker image

2. generate genesis proof, genesis ledger with the input runtime configuration using `runtime_genesis_ledger.exe` and uplod it to s3 (TODO: or pass the configuration file and let the nodes generate one themselves? if we are having them download then might as well download the ledger and the proof)
 
```ocaml

    module Test_executive 
        (Test: sig
            val execute : module Log_engine -> Testnet.t -> Runtime_config.t -> bool Deferred.t
            end)
        ( Config : sig
            val runtime_config : Runtime_config.t
            val network_config: Network_config.t
            end) =
    struct  =

        let execute () =
            let deploy_config = {daemon_image}
            in
            let%bind testnet: Testnet.t = Cloud_network_manager.deploy network_config daemon_config
            in
            let module Log_engine = Make_log_engine (struct
                let testnet = testnet
                end)
            in
            let%bind result = Test.execute Log_engine testnet runtime_config
            in
            let%map () = Cloud_network_manager.destroy testnet
            in
            if not result then failwith "Test failed" else ()

    end
```

## Outstanding Questions/Concerns

1. We'd still need build profiles because of the constraint constants.
2. Would we be running all the tests with snarks?
3. Snark coordinators currently don't sync with each other which makes it a waste of resource to allow multiple coordinators
